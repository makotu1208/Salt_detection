{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization,multiply\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate,add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "\n",
    "import tensorflow as tf\n",
    "import CyclicLearningRate\n",
    "import pydot\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img#,save_img\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "im_width = 101\n",
    "im_height = 101\n",
    "im_chan = 1\n",
    "basicpath = '../input/'\n",
    "path_train = basicpath + 'images/train/'\n",
    "path_test = basicpath + 'images/test/'\n",
    "\n",
    "path_train_images = path_train + 'images/'\n",
    "path_train_masks = path_train + 'masks/'\n",
    "path_test_images = path_test + 'images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_depth_train_1018_adam_v1.model\n",
      "final_depth_train_1018_adam_v1.csv\n"
     ]
    }
   ],
   "source": [
    "cv_total = 5\n",
    "#cv_index = 1 -5\n",
    "seed = 8234\n",
    "\n",
    "version = 1\n",
    "basic_name_ori = 'final_depth_train_1018_adam_v%s' % (version)\n",
    "save_model_name = basic_name_ori + '.model'\n",
    "submission_file = basic_name_ori + '.csv'\n",
    "\n",
    "print(save_model_name)\n",
    "print(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69eefe126b24da59be6ee4ba0c4f1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0e7bcb90fc4189bbad462cd981a91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "# load Images\n",
    "train_df[\"images\"] = [np.array(load_img(\"../input/images/train/images/{}.png\".format(idx), color_mode = \"grayscale\")) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"../input/images/train/masks/{}.png\".format(idx), color_mode = \"grayscale\")) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_type(mask):\n",
    "    border = 10\n",
    "    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n",
    "    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n",
    "\n",
    "    cover = (mask>0.5).sum()\n",
    "    if cover < 8:\n",
    "        return 0 # empty\n",
    "    if cover == ((mask*outer) > 0.5).sum():\n",
    "        return 1 #border\n",
    "    if np.all(mask==mask[0]):\n",
    "        return 2 #vertical\n",
    "\n",
    "    percentage = cover/(101*101)\n",
    "    if percentage < 0.15:\n",
    "        return 3\n",
    "    elif percentage < 0.25:\n",
    "        return 4\n",
    "    elif percentage < 0.50:\n",
    "        return 5\n",
    "    elif percentage < 0.75:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def histcoverage(coverage):\n",
    "    histall = np.zeros((1,8))\n",
    "    for c in coverage:\n",
    "        histall[0,c] += 1\n",
    "    return histall\n",
    "\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_target, 2)\n",
    "train_df[\"coverage_class\"] = train_df.masks.map(get_mask_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3198,) (802,)\n",
      "(3199,) (801,)\n",
      "(3199,) (801,)\n",
      "(3200,) (800,)\n",
      "(3204,) (796,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "train_all = []\n",
    "evaluate_all = []\n",
    "skf = StratifiedKFold(n_splits=cv_total, random_state=seed, shuffle=True)\n",
    "for train_index, evaluate_index in skf.split(train_df.index.values, train_df.coverage_class):\n",
    "    train_all.append(train_index)\n",
    "    evaluate_all.append(evaluate_index)\n",
    "    print(train_index.shape,evaluate_index.shape) # the shape is slightly different in different cv, it's OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_data(cv_index):\n",
    "    train_index = train_all[cv_index-1]\n",
    "    evaluate_index = evaluate_all[cv_index-1]\n",
    "    x_train = np.array(train_df.images[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    y_train = np.array(train_df.masks[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    x_valid = np.array(train_df.images[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    y_valid = np.array(train_df.masks[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    return x_train,y_train,x_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size, seq_det):\n",
    "    # create empty arrays to contain batch of features and labels\n",
    "    batch_features = np.zeros((batch_size, features.shape[1], features.shape[2], features.shape[3]))\n",
    "    batch_labels = np.zeros((batch_size, labels.shape[1], labels.shape[2], labels.shape[3]))\n",
    "\n",
    "    while True:\n",
    "        # Fill arrays of batch size with augmented data taken randomly from full passed arrays\n",
    "        indexes = random.sample(range(len(features)), batch_size)\n",
    "        # Perform the exactly the same augmentation for X and y\n",
    "        random_augmented_images, random_augmented_labels = do_augmentation(seq_det, features[indexes], labels[indexes])\n",
    "        batch_features[:,:,:,:] = random_augmented_images[:,:,:,:]\n",
    "        batch_labels[:,:,:,:] = random_augmented_labels[:,:,:,:]\n",
    "\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune\n",
    "seq = iaa.Sometimes(0.75,\n",
    "                      iaa.Noop(),\n",
    "                           iaa.Sometimes(0.5,          \n",
    "                             iaa.OneOf([\n",
    "                                    iaa.PerspectiveTransform(scale=(0.02, 0.07)),\n",
    "                                    iaa.ElasticTransformation(alpha=(0.98, 1.08), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                    iaa.Affine(rotate=(-10, 10),order=1,mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                    iaa.Affine(order=1,translate_percent={\"x\": (-0.1, 0.1)},mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                    iaa.Affine(order=1,shear=(-10, 10), mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                    iaa.Affine(rotate=(-8, 8),translate_percent={\"x\": (-0.08, 0.08)},shear=(-8, 8),order=1,mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                    iaa.PiecewiseAffine(scale=(0.01, 0.07), mode=[\"edge\"], cval=(0)),\n",
    "                                    iaa.CropAndPad(percent=(-0.1, 0),pad_mode=[\"edge\"]),\n",
    "                                    iaa.GaussianBlur(sigma=(0.01, 0.6)),\n",
    "                                    iaa.Multiply((0.9, 1.1)),\n",
    "                                    iaa.ContrastNormalization((0.8, 1.2))\n",
    "\n",
    "                              ]),\n",
    "                            iaa.Sometimes(0.5, \n",
    "                                iaa.SomeOf(2,[\n",
    "                                    iaa.PerspectiveTransform(scale=(0.02, 0.07)),\n",
    "                                    iaa.ElasticTransformation(alpha=(0.98, 1.08), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                    iaa.OneOf([\n",
    "                                        iaa.Affine(rotate=(-10, 10),order=1,mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                        iaa.Affine(order=1,translate_percent={\"x\": (-0.1, 0.1)},mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                        iaa.Affine(order=1,shear=(-8, 8), mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                        iaa.Affine(rotate=(-10, 10),translate_percent={\"x\": (-0.1, 0.1)},shear=(-8, 8),order=1,mode=[\"edge\"], cval=(0),backend = \"skimage\")\n",
    "                                    ]),\n",
    "                                    iaa.PiecewiseAffine(scale=(0.01, 0.07), mode=[\"edge\"], cval=(0)),\n",
    "                                    iaa.CropAndPad(percent=(-0.1, 0),pad_mode=[\"edge\"]),\n",
    "                                    iaa.GaussianBlur(sigma=(0.01, 0.6)),\n",
    "                                    iaa.Multiply((0.9, 1.1)),\n",
    "                                    iaa.ContrastNormalization((0.8, 1.2))\n",
    "                              ],random_order=True),\n",
    "                              iaa.Sequential([\n",
    "                                  iaa.OneOf([\n",
    "                                            iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "                                            iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                          iaa.Sometimes(0.4,\n",
    "                                            iaa.OneOf([\n",
    "                                                iaa.Affine(rotate=(-10, 10),order=1,mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                                iaa.Affine(order=1,translate_percent={\"x\": (-0.1, 0.1)},mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                                iaa.Affine(order=1,shear=(-8, 8), mode=[\"edge\"], cval=(0),backend = \"skimage\"),\n",
    "                                              ]),\n",
    "                                                iaa.Affine(rotate=(-10, 10),translate_percent={\"x\": (-0.1, 0.1)},shear=(-8, 8),order=1,mode=[\"edge\"], cval=(0),backend = \"skimage\")\n",
    "                                                ),\n",
    "                                            iaa.PiecewiseAffine(scale=(0.01, 0.07), mode=[\"edge\"], cval=(0)),\n",
    "                                            iaa.CropAndPad(percent=(-0.1, 0),pad_mode=[\"edge\"])\n",
    "                                  ]),\n",
    "                                    iaa.OneOf([iaa.GaussianBlur(sigma=(0.01, 0.5)),\n",
    "                                               iaa.Multiply((0.9, 1.1)),\n",
    "                                               iaa.ContrastNormalization((0.8, 1.2))\n",
    "                                              ])\n",
    "                                              ])\n",
    "                   )\n",
    "               )\n",
    "                   )        \n",
    "            \n",
    "\n",
    "\n",
    "seq_det = seq.to_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# more tune\\nseq = iaa.Sometimes(0.5,\\n                      iaa.Noop(),\\n                      iaa.Sometimes(0.75,\\n                          iaa.OneOf([\\n                                iaa.PerspectiveTransform(scale=(0.04, 0.08)),\\n                                iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\\n                                iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode=\\'symmetric\\', cval=(0),backend = \"skimage\"),\\n                                iaa.PiecewiseAffine(scale=(0.05, 0.1), mode=\\'edge\\', cval=(0)),\\n                                iaa.CropAndPad(percent=(-0.1, 0)),\\n                                iaa.OneOf([iaa.GaussianBlur(sigma=(0.0, 1.0)),\\n                                           iaa.AverageBlur(k=(2, 5)),\\n                                           iaa.MedianBlur(k=(3, 5)),\\n                                           iaa.Multiply((0.95, 1.05)),\\n                                           ]),\\n                            ]),\\n                          iaa.Sequential([\\n                              iaa.OneOf([\\n                                        iaa.PerspectiveTransform(scale=(0.04, 0.08)),\\n                                        iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\\n                                        iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode=\\'symmetric\\', cval=(0),backend = \"skimage\"),\\n                                        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode=\\'edge\\', cval=(0)),\\n                                        iaa.CropAndPad(percent=(-0.1, 0))\\n                              ]),\\n                              iaa.OneOf([\\n                                iaa.OneOf([iaa.GaussianBlur(sigma=(0.0, 1.0)),\\n                                           iaa.AverageBlur(k=(2, 5)),\\n                                           iaa.MedianBlur(k=(3, 5))\\n                                          ]),\\n                                           iaa.Multiply((0.95, 1.05)),\\n                                           iaa.ContrastNormalization((0.8, 1.2))\\n                              ])\\n                              ])\\n                          )\\n                   )\\n\\nseq_det = seq.to_deterministic()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# more tune\n",
    "seq = iaa.Sometimes(0.5,\n",
    "                      iaa.Noop(),\n",
    "                      iaa.Sometimes(0.75,\n",
    "                          iaa.OneOf([\n",
    "                                iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "                                iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0),backend = \"skimage\"),\n",
    "                                iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0)),\n",
    "                                iaa.CropAndPad(percent=(-0.1, 0)),\n",
    "                                iaa.OneOf([iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "                                           iaa.AverageBlur(k=(2, 5)),\n",
    "                                           iaa.MedianBlur(k=(3, 5)),\n",
    "                                           iaa.Multiply((0.95, 1.05)),\n",
    "                                           ]),\n",
    "                            ]),\n",
    "                          iaa.Sequential([\n",
    "                              iaa.OneOf([\n",
    "                                        iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "                                        iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                        iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0),backend = \"skimage\"),\n",
    "                                        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0)),\n",
    "                                        iaa.CropAndPad(percent=(-0.1, 0))\n",
    "                              ]),\n",
    "                              iaa.OneOf([\n",
    "                                iaa.OneOf([iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "                                           iaa.AverageBlur(k=(2, 5)),\n",
    "                                           iaa.MedianBlur(k=(3, 5))\n",
    "                                          ]),\n",
    "                                           iaa.Multiply((0.95, 1.05)),\n",
    "                                           iaa.ContrastNormalization((0.8, 1.2))\n",
    "                              ])\n",
    "                              ])\n",
    "                          )\n",
    "                   )\n",
    "\n",
    "seq_det = seq.to_deterministic()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Best:0.799\\nseq = iaa.Sometimes(0.4,\\n                      iaa.Noop(),\\n                      iaa.Sometimes(0.7,\\n                          iaa.OneOf([\\n                            iaa.OneOf([\\n                                iaa.PerspectiveTransform(scale=(0.04, 0.08)),\\n                                iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\\n                                iaa.OneOf([\\n                                    iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode=\\'symmetric\\', cval=(0),backend = \"skimage\"),\\n                                    iaa.PiecewiseAffine(scale=(0.05, 0.1), mode=\\'edge\\', cval=(0))])]),\\n                            iaa.OneOf([iaa.CropAndPad(percent=(-0.1, 0))]),\\n                            iaa.OneOf([\\n                                iaa.GaussianBlur(sigma=(0.0, 1.0)),\\n                                iaa.AverageBlur(k=(2, 5)),\\n                                iaa.MedianBlur(k=(3, 5))]),\\n                            iaa.OneOf([\\n                                iaa.Add((-10, 10)),\\n                                iaa.Invert(0.15),\\n                                iaa.Multiply((0.95, 1.05)),\\n                                #iaa.MultiplyElementwise((0.95, 1.05)),\\n                                iaa.ContrastNormalization((0.8, 1.2))])\\n                            ]),\\n                      iaa.Sequential([\\n                          iaa.SomeOf((1,2),[\\n                                iaa.OneOf([\\n                                    iaa.PerspectiveTransform(scale=(0.04, 0.08)),\\n                                    iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\\n                                    iaa.OneOf([\\n                                        iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode=\\'symmetric\\', cval=(0),backend = \"skimage\"),\\n                                        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode=\\'edge\\', cval=(0))])]),\\n                                iaa.CropAndPad(percent=(-0.1, 0))],random_order=True),\\n                          iaa.OneOf([\\n                             iaa.OneOf([\\n                                iaa.GaussianBlur(sigma=(0.0, 1.0)),\\n                                iaa.AverageBlur(k=(2, 5)),\\n                                iaa.MedianBlur(k=(3, 5))]),\\n                             iaa.OneOf([\\n                                iaa.Add((-10, 10)),\\n                                iaa.Invert(0.15),\\n                                iaa.Multiply((0.95, 1.05)),\\n                                #iaa.MultiplyElementwise((0.95, 1.05)),\\n                                iaa.ContrastNormalization((0.7, 1.3))])\\n                         ])\\n                      ],random_order=True)\\n                                   )\\n                     )\\n\\nseq_det = seq.to_deterministic()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Best:0.799\n",
    "seq = iaa.Sometimes(0.4,\n",
    "                      iaa.Noop(),\n",
    "                      iaa.Sometimes(0.7,\n",
    "                          iaa.OneOf([\n",
    "                            iaa.OneOf([\n",
    "                                iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "                                iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                iaa.OneOf([\n",
    "                                    iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0),backend = \"skimage\"),\n",
    "                                    iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0))])]),\n",
    "                            iaa.OneOf([iaa.CropAndPad(percent=(-0.1, 0))]),\n",
    "                            iaa.OneOf([\n",
    "                                iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "                                iaa.AverageBlur(k=(2, 5)),\n",
    "                                iaa.MedianBlur(k=(3, 5))]),\n",
    "                            iaa.OneOf([\n",
    "                                iaa.Add((-10, 10)),\n",
    "                                iaa.Invert(0.15),\n",
    "                                iaa.Multiply((0.95, 1.05)),\n",
    "                                #iaa.MultiplyElementwise((0.95, 1.05)),\n",
    "                                iaa.ContrastNormalization((0.8, 1.2))])\n",
    "                            ]),\n",
    "                      iaa.Sequential([\n",
    "                          iaa.SomeOf((1,2),[\n",
    "                                iaa.OneOf([\n",
    "                                    iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "                                    iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\"),\n",
    "                                    iaa.OneOf([\n",
    "                                        iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0),backend = \"skimage\"),\n",
    "                                        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0))])]),\n",
    "                                iaa.CropAndPad(percent=(-0.1, 0))],random_order=True),\n",
    "                          iaa.OneOf([\n",
    "                             iaa.OneOf([\n",
    "                                iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "                                iaa.AverageBlur(k=(2, 5)),\n",
    "                                iaa.MedianBlur(k=(3, 5))]),\n",
    "                             iaa.OneOf([\n",
    "                                iaa.Add((-10, 10)),\n",
    "                                iaa.Invert(0.15),\n",
    "                                iaa.Multiply((0.95, 1.05)),\n",
    "                                #iaa.MultiplyElementwise((0.95, 1.05)),\n",
    "                                iaa.ContrastNormalization((0.7, 1.3))])\n",
    "                         ])\n",
    "                      ],random_order=True)\n",
    "                                   )\n",
    "                     )\n",
    "\n",
    "seq_det = seq.to_deterministic()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nseq = iaa.Sometimes(0.5,iaa.OneOf([\\n        iaa.GaussianBlur(sigma=(0.0, 1.0)),\\n        iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode=\\'symmetric\\', cval=(0),backend = \"skimage\"),\\n        iaa.CropAndPad(percent=(-0.1, 0)),\\n        iaa.PerspectiveTransform(scale=(0.04, 0.08)),\\n        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode=\\'edge\\', cval=(0)),\\n        iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\")\\n        ]))\\nseq_det = seq.to_deterministic()\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "seq = iaa.Sometimes(0.5,iaa.OneOf([\n",
    "        iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "        iaa.Affine(rotate=(-10, 10),order=1,translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0),backend = \"skimage\"),\n",
    "        iaa.CropAndPad(percent=(-0.1, 0)),\n",
    "        iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0)),\n",
    "        iaa.ElasticTransformation(alpha=(0.95, 1.05), sigma=0.25,mode=\"nearest\",name = \"elastic\")\n",
    "        ]))\n",
    "seq_det = seq.to_deterministic()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bb2f7a2e214b0fa9049d604e45faef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=128), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef do_augmentation(seq_det, X_train, y_train):\\n    # Move from 0-1 float to uint8 format (needed for most imgaug operators)\\n    X_train_aug = [(x[:,:,:] * 255.0).astype(np.uint8) for x in X_train]\\n    # Do augmentation\\n    X_train_aug = seq_det.augment_images(X_train_aug)\\n    # Back to 0-1 float range\\n    X_train_aug = np.array([(x[:,:,:].astype(np.float64)) / 255.0 for x in X_train_aug])\\n    \\n    X_train_depth = X_train_aug[:,:,:,0].reshape(-1,128,128,1) * empty\\n    X_train_aug[:,:,:,1] = empty[:,:,:,0]\\n    X_train_aug[:,:,:,2] = X_train_depth[:,:,:,0]\\n\\n    \\n    # Move from 0-1 float to uint8 format (needed for imgaug)\\n    y_train_aug = [(x[:,:,:] * 255.0).astype(np.uint8) for x in y_train]\\n    # Do augmentation\\n    y_train_aug = seq_det.augment_images(y_train_aug)\\n    # Make sure we only have 2 values for mask augmented\\n    y_train_aug = [np.where(x[:,:,:] > 0, 255, 0) for x in y_train_aug]\\n    # Back to 0-1 float range\\n    y_train_aug = [(x[:,:,:].astype(np.float64)) / 255.0 for x in y_train_aug]\\n    return X_train_aug, np.array(y_train_aug)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add depth_channel\n",
    "# define batch_size\n",
    "batch_size = 128 # maybe 128 is best\n",
    "\n",
    "empty = np.zeros((batch_size,128,128,1))\n",
    "for i in tqdm_notebook(range(len(empty))):\n",
    "    for row, const in enumerate(np.linspace(0, 1, 128)):\n",
    "        empty[i, row, :] = const\n",
    "\n",
    "\n",
    "        \n",
    "def do_augmentation(seq_det, X_train, y_train):\n",
    "    # Move from 0-1 float to uint8 format (needed for most imgaug operators)\n",
    "    X_train_aug = [(x[:,:,:] * 255.0).astype(np.uint8) for x in X_train]\n",
    "    # Do augmentation\n",
    "    X_train_aug = seq_det.augment_images(X_train_aug)\n",
    "    # Back to 0-1 float range\n",
    "    X_train_aug = np.array([(x[:,:,:].astype(np.float64)) / 255.0 for x in X_train_aug])\n",
    "    \n",
    "    X_train_depth = X_train_aug[:,:,:,0].reshape(-1,128,128,1) * empty\n",
    "    X_train_aug[:,:,:,1] = empty[:,:,:,0]\n",
    "    X_train_aug[:,:,:,2] = X_train_depth[:,:,:,0]\n",
    "\n",
    "    \n",
    "    # Move from 0-1 float to uint8 format (needed for imgaug)\n",
    "    y_train_aug = [(x[:,:,:] * 255.0).astype(np.uint8) for x in y_train]\n",
    "    # Do augmentation\n",
    "    y_train_aug = seq_det.augment_images(y_train_aug)\n",
    "    # Make sure we only have 2 values for mask augmented\n",
    "    y_train_aug = [np.where(x[:,:,:] > 0, 255, 0) for x in y_train_aug]\n",
    "    # Back to 0-1 float range\n",
    "    y_train_aug = [(x[:,:,:].astype(np.float64)) / 255.0 for x in y_train_aug]\n",
    "    return X_train_aug, np.array(y_train_aug)\n",
    "        \n",
    "        \n",
    "'''\n",
    "def do_augmentation(seq_det, X_train, y_train):\n",
    "    # Move from 0-1 float to uint8 format (needed for most imgaug operators)\n",
    "    X_train_aug = [(x[:,:,:] * 255.0).astype(np.uint8) for x in X_train]\n",
    "    # Do augmentation\n",
    "    X_train_aug = seq_det.augment_images(X_train_aug)\n",
    "    # Back to 0-1 float range\n",
    "    X_train_aug = np.array([(x[:,:,:].astype(np.float64)) / 255.0 for x in X_train_aug])\n",
    "    \n",
    "    X_train_depth = X_train_aug[:,:,:,0].reshape(-1,128,128,1) * empty\n",
    "    X_train_aug[:,:,:,1] = empty[:,:,:,0]\n",
    "    X_train_aug[:,:,:,2] = X_train_depth[:,:,:,0]\n",
    "\n",
    "    \n",
    "    # Move from 0-1 float to uint8 format (needed for imgaug)\n",
    "    y_train_aug = [(x[:,:,:] * 255.0).astype(np.uint8) for x in y_train]\n",
    "    # Do augmentation\n",
    "    y_train_aug = seq_det.augment_images(y_train_aug)\n",
    "    # Make sure we only have 2 values for mask augmented\n",
    "    y_train_aug = [np.where(x[:,:,:] > 0, 255, 0) for x in y_train_aug]\n",
    "    # Back to 0-1 float range\n",
    "    y_train_aug = [(x[:,:,:].astype(np.float64)) / 255.0 for x in y_train_aug]\n",
    "    return X_train_aug, np.array(y_train_aug)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build resnet 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_block_names_old(stage):\n",
    "    conv_name = 'decoder_stage{}_conv'.format(stage)\n",
    "    bn_name = 'decoder_stage{}_bn'.format(stage)\n",
    "    relu_name = 'decoder_stage{}_relu'.format(stage)\n",
    "    up_name = 'decoder_stage{}_upsample'.format(stage)\n",
    "    return conv_name, bn_name, relu_name, up_name\n",
    "\n",
    "\n",
    "def Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                     batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names_old(stage)\n",
    "\n",
    "        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate()([x, skip])\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'1')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "        x = Activation('relu', name=relu_name+'1')(x)\n",
    "\n",
    "        x = Dropout(0.3)(x) # add\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'2')(x)\n",
    "        x = Activation('relu', name=relu_name+'2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                      transpose_kernel_size=(4,4), batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names_old(stage)\n",
    "\n",
    "        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n",
    "                            padding='same', name=up_name)(input_tensor)\n",
    "        \n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Activation('relu', name=relu_name+'1')(x)\n",
    "            x = Concatenate()([x, skip])\n",
    "            x = Dropout(0.5)(x) # change 0.3â†’0.5\n",
    "            x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n",
    "        \n",
    "        #if batchnorm: del\n",
    "        #x = BatchNormalization(name=bn_name+'2')(x) del\n",
    "        #x = Activation('relu', name=relu_name+'2')(x) del\n",
    "\n",
    "        return x\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_params(**params):\n",
    "    default_conv_params = {\n",
    "        'kernel_initializer': 'glorot_uniform',\n",
    "        'use_bias': False,\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "    default_conv_params.update(params)\n",
    "    return default_conv_params\n",
    "\n",
    "def get_bn_params(**params):\n",
    "    default_bn_params = {\n",
    "        'axis': 3,\n",
    "        'momentum': 0.99,\n",
    "        'epsilon': 2e-5,\n",
    "        'center': True,\n",
    "        'scale': True,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_block_names(stage, block):\n",
    "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
    "    conv_name = name_base + 'conv'\n",
    "    bn_name = name_base + 'bn'\n",
    "    relu_name = name_base + 'relu'\n",
    "    sc_name = name_base + 'sc'\n",
    "    return conv_name, bn_name, relu_name, sc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION = \"relu\"\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #if activation == True:\n",
    "    #    x = Activation(ACTIVATION)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=64):\n",
    "    \n",
    "    x = BatchNormalization()(blockInput)\n",
    "    x = Activation(ACTIVATION)(x)\n",
    "    x = convolution_block(x, num_filters, (3,3) ,activation=True)\n",
    "    x = BatchNormalization()(blockInput)\n",
    "    x = Activation(ACTIVATION)(x)\n",
    "    x = convolution_block(x, num_filters, (3,3),activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    \n",
    "    '''\n",
    "    x = Activation(ACTIVATION)(blockInput)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    '''\n",
    "    return x\n",
    "\n",
    "# add SE_block\n",
    "def se_block(blockInput, num_filters=16, ratio=8):\n",
    "    x = GlobalAveragePooling2D()(blockInput) \n",
    "    x = Dense(num_filters//ratio, activation='relu')(x) \n",
    "    x = Dense(num_filters, activation='sigmoid')(x) \n",
    "    return multiply([blockInput, x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_identity_block(filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n",
    "        \n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usual_conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def usual_identity_block(filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(backbone, classes, last_block_filters, skip_layers,\n",
    "               n_upsample_blocks=5, upsample_rates=(2,2,2,2,2),\n",
    "               block_type='upsampling', activation='sigmoid',\n",
    "               **kwargs):\n",
    "\n",
    "    input = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    if block_type == 'transpose':\n",
    "        up_block = Transpose2D_block\n",
    "    else:\n",
    "        up_block = Upsample2D_block\n",
    "\n",
    "    # convert layer names to indices\n",
    "    skip_layers = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n",
    "                    for l in skip_layers])\n",
    "    #print(skip_layers)\n",
    "    #print(\"n_upsample_blocks: \",n_upsample_blocks)\n",
    "    for i in range(n_upsample_blocks):\n",
    "\n",
    "        # check if there is a skip connection\n",
    "        if i < len(skip_layers):\n",
    "            #print(backbone.layers[skip_layers[i]])\n",
    "            #print(backbone.layers[skip_layers[i]].output)\n",
    "            skip = backbone.layers[skip_layers[i]].output\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        up_size = (upsample_rates[i], upsample_rates[i])\n",
    "        filters = last_block_filters * 2**(n_upsample_blocks-(i+1))\n",
    "\n",
    "        x = up_block(filters, i, upsample_rate=up_size, skip=skip, **kwargs)(x)\n",
    "        \n",
    "        #if i < len(skip_layers):\n",
    "            #x = residual_block(x,filters) #add\n",
    "            #x = residual_block(x,filters) #add\n",
    "            #x = se_block(x,filters,ratio = filters//2) #add\n",
    "            #x = SpatialDropout2D(0.5)(x)\n",
    "        \n",
    "    if classes < 2:\n",
    "        activation = 'sigmoid'\n",
    "    \n",
    "    #x = SpatialDropout2D(0.2)(x) # add\n",
    "    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n",
    "    x = Activation(activation, name=activation)(x)\n",
    "\n",
    "    model = Model(input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.engine import get_source_inputs\n",
    "\n",
    "import keras\n",
    "from distutils.version import StrictVersion\n",
    "\n",
    "if StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n",
    "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "else:\n",
    "    from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "def build_resnet(\n",
    "     repetitions=(2, 2, 2, 2),\n",
    "     include_top=True,\n",
    "     input_tensor=None,\n",
    "     input_shape=None,\n",
    "     classes=1000,\n",
    "     block_type='usual'):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=101,\n",
    "                                      data_format='channels_last',\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape, name='data')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "            \n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "    init_filters = 64\n",
    "\n",
    "    if block_type == 'basic':\n",
    "        conv_block = basic_conv_block\n",
    "        identity_block = basic_identity_block\n",
    "    else:\n",
    "        conv_block = usual_conv_block\n",
    "        identity_block = usual_identity_block\n",
    "    \n",
    "    # resnet bottom\n",
    "    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n",
    "    x = BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu0')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n",
    "    \n",
    "    # resnet body\n",
    "    for stage, rep in enumerate(repetitions):\n",
    "        for block in range(rep):\n",
    "            \n",
    "            filters = init_filters * (2**stage)\n",
    "            \n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if block == 0 and stage == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n",
    "                \n",
    "            elif block == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n",
    "                \n",
    "            else:\n",
    "                x = identity_block(filters, stage, block)(x)\n",
    "                \n",
    "    x = BatchNormalization(name='bn1', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "\n",
    "    # resnet top\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D(name='pool1')(x)\n",
    "        x = Dense(classes, name='fc1')(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "        \n",
    "    # Create model.\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_collection = [\n",
    "    # ResNet34\n",
    "    {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': True,\n",
    "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n",
    "        'name': 'resnet34_imagenet_1000.h5',\n",
    "        'md5': '2ac8277412f65e5d047f255bcbd10383',\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': False,\n",
    "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n",
    "        'name': 'resnet34_imagenet_1000_no_top.h5',\n",
    "        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n",
    "    model = build_resnet(input_tensor=input_tensor,\n",
    "                         input_shape=input_shape,\n",
    "                         repetitions=(3, 4, 6, 3),\n",
    "                         classes=classes,\n",
    "                         include_top=include_top,\n",
    "                         block_type='basic')\n",
    "    model.name = 'resnet34'\n",
    "\n",
    "    if weights:\n",
    "        load_model_weights(weights_collection, model, weights, classes, include_top)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "\n",
    "\n",
    "def find_weights(weights_collection, model_name, dataset, include_top):\n",
    "    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n",
    "    w = list(filter(lambda x: x['dataset'] == dataset, w))\n",
    "    w = list(filter(lambda x: x['include_top'] == include_top, w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def load_model_weights(weights_collection, model, dataset, classes, include_top):\n",
    "    weights = find_weights(weights_collection, model.name, dataset, include_top)\n",
    "\n",
    "    if weights:\n",
    "        weights = weights[0]\n",
    "\n",
    "        if include_top and weights['classes'] != classes:\n",
    "            raise ValueError('If using `weights` and `include_top`'\n",
    "                             ' as true, `classes` should be {}'.format(weights['classes']))\n",
    "\n",
    "        weights_path = get_file(weights['name'],\n",
    "                                weights['url'],\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash=weights['md5'])\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('There is no weights for such configuration: ' +\n",
    "                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n",
    "                         'classes = {}, include_top = {}.'.format(classes, include_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UResNet34(input_shape=(None, None, 3), classes=1, decoder_filters=16, decoder_block_type='transpose',\n",
    "                       encoder_weights=None, input_tensor=None, activation='sigmoid', **kwargs):\n",
    "\n",
    "#     backbone = ResnetBuilder.build_resnet_34(input_shape=input_shape,input_tensor=input_tensor)\n",
    "    backbone = ResNet34(input_shape=input_shape, weights='imagenet', classes=1000,include_top=False)\n",
    "    skip_connections = list([106,74,37,5])  # for resnet 34\n",
    "    model = build_unet(backbone, classes, decoder_filters,\n",
    "                       skip_connections, block_type=decoder_block_type,\n",
    "                       activation=activation, **kwargs)\n",
    "    model.name = 'u-resnet34'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = UResNet34(input_shape=(128,128,3))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom IPython.display import SVG\\nfrom keras.utils.vis_utils import model_to_dot\\n\\nplot_model(model, to_file='model_20181008_v5.png')\\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "plot_model(model, to_file='model_20181008_v5.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_vector(A, B):\n",
    "    A = np.squeeze(A) # new added \n",
    "    B = np.squeeze(B) # new added\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch]>0, B[batch]>0\n",
    "        if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n",
    "            metric.append(0)\n",
    "            continue\n",
    "        if np.count_nonzero(t) >= 1 and np.count_nonzero(p) == 0:\n",
    "            metric.append(0)\n",
    "            continue\n",
    "        if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n",
    "            metric.append(1)\n",
    "            continue\n",
    "        \n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t, p)\n",
    "        iou = (np.sum(intersection > 0)  )/ (np.sum(union > 0) )\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "\n",
    "    return np.mean(metric)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code download from: https://github.com/bermanmaxim/LovaszSoftmax\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
    "    #logits = K.log(y_pred / (1. - y_pred))\n",
    "    logits = y_pred #Jiaxin\n",
    "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = weight * (logit_y_pred * (1. - y_true) + \n",
    "                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA only lr flip\n",
    "def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n",
    "    print('pred_1 start...')\n",
    "    preds_test = model.predict([x_test]).reshape(-1, img_size_target, img_size_target)\n",
    "        \n",
    "    print('pred_ref start...')\n",
    "    preds_test_reflect = np.array([np.fliplr(x) for x in model.predict([np.array([np.fliplr(x) for x in x_test])]).reshape(-1, img_size_target, img_size_target)])\n",
    "    preds_test += preds_test_reflect\n",
    "    \n",
    "    del preds_test_reflect\n",
    "    gc.collect()\n",
    "    \n",
    "    return preds_test / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      " train_1010_adam_depth_aug_v3_v3_cv1\n",
      "make cv data...\n",
      "start data aug...\n",
      "start add channel...\n",
      "start bulid model...\n",
      "start fit_model...\n",
      "Epoch 1/200\n",
      "200/200 [==============================] - 53s 264ms/step - loss: 0.8736 - my_iou_metric: 0.2166 - val_loss: 1.0665 - val_my_iou_metric: 0.2581\n",
      "\n",
      "Epoch 00001: val_my_iou_metric improved from -inf to 0.25810, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.5381 - my_iou_metric: 0.4885 - val_loss: 0.5597 - val_my_iou_metric: 0.6066\n",
      "\n",
      "Epoch 00002: val_my_iou_metric improved from 0.25810 to 0.60661, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.4768 - my_iou_metric: 0.5992 - val_loss: 0.3371 - val_my_iou_metric: 0.6882\n",
      "\n",
      "Epoch 00003: val_my_iou_metric improved from 0.60661 to 0.68815, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.4312 - my_iou_metric: 0.6503 - val_loss: 0.3777 - val_my_iou_metric: 0.6954\n",
      "\n",
      "Epoch 00004: val_my_iou_metric improved from 0.68815 to 0.69539, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.3629 - my_iou_metric: 0.6819 - val_loss: 0.3535 - val_my_iou_metric: 0.6986\n",
      "\n",
      "Epoch 00005: val_my_iou_metric improved from 0.69539 to 0.69863, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.3407 - my_iou_metric: 0.7001 - val_loss: 0.4324 - val_my_iou_metric: 0.6592\n",
      "\n",
      "Epoch 00006: val_my_iou_metric did not improve from 0.69863\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.3118 - my_iou_metric: 0.7241 - val_loss: 0.3400 - val_my_iou_metric: 0.7014\n",
      "\n",
      "Epoch 00007: val_my_iou_metric improved from 0.69863 to 0.70137, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.2698 - my_iou_metric: 0.7428 - val_loss: 0.2968 - val_my_iou_metric: 0.7388\n",
      "\n",
      "Epoch 00008: val_my_iou_metric improved from 0.70137 to 0.73878, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.2420 - my_iou_metric: 0.7604 - val_loss: 0.3788 - val_my_iou_metric: 0.7247\n",
      "\n",
      "Epoch 00009: val_my_iou_metric did not improve from 0.73878\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.2217 - my_iou_metric: 0.7660 - val_loss: 0.4002 - val_my_iou_metric: 0.6888\n",
      "\n",
      "Epoch 00010: val_my_iou_metric did not improve from 0.73878\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.2224 - my_iou_metric: 0.7725 - val_loss: 0.3087 - val_my_iou_metric: 0.7490\n",
      "\n",
      "Epoch 00011: val_my_iou_metric improved from 0.73878 to 0.74900, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.2026 - my_iou_metric: 0.7787 - val_loss: 0.3241 - val_my_iou_metric: 0.7347\n",
      "\n",
      "Epoch 00012: val_my_iou_metric did not improve from 0.74900\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.1785 - my_iou_metric: 0.7890 - val_loss: 0.3083 - val_my_iou_metric: 0.7251\n",
      "\n",
      "Epoch 00013: val_my_iou_metric did not improve from 0.74900\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1646 - my_iou_metric: 0.8044 - val_loss: 0.2671 - val_my_iou_metric: 0.7665\n",
      "\n",
      "Epoch 00014: val_my_iou_metric improved from 0.74900 to 0.76646, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1648 - my_iou_metric: 0.7978 - val_loss: 0.3019 - val_my_iou_metric: 0.7385\n",
      "\n",
      "Epoch 00015: val_my_iou_metric did not improve from 0.76646\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.1600 - my_iou_metric: 0.7997 - val_loss: 0.2845 - val_my_iou_metric: 0.7675\n",
      "\n",
      "Epoch 00016: val_my_iou_metric improved from 0.76646 to 0.76746, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1608 - my_iou_metric: 0.7978 - val_loss: 0.2864 - val_my_iou_metric: 0.7549\n",
      "\n",
      "Epoch 00017: val_my_iou_metric did not improve from 0.76746\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1493 - my_iou_metric: 0.7992 - val_loss: 0.3160 - val_my_iou_metric: 0.7754\n",
      "\n",
      "Epoch 00018: val_my_iou_metric improved from 0.76746 to 0.77544, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1616 - my_iou_metric: 0.8082 - val_loss: 0.3204 - val_my_iou_metric: 0.7483\n",
      "\n",
      "Epoch 00019: val_my_iou_metric did not improve from 0.77544\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1510 - my_iou_metric: 0.8089 - val_loss: 0.2650 - val_my_iou_metric: 0.7608\n",
      "\n",
      "Epoch 00020: val_my_iou_metric did not improve from 0.77544\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1503 - my_iou_metric: 0.8095 - val_loss: 0.2856 - val_my_iou_metric: 0.7709\n",
      "\n",
      "Epoch 00021: val_my_iou_metric did not improve from 0.77544\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.1331 - my_iou_metric: 0.8157 - val_loss: 0.3088 - val_my_iou_metric: 0.7670\n",
      "\n",
      "Epoch 00022: val_my_iou_metric did not improve from 0.77544\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.1391 - my_iou_metric: 0.8201 - val_loss: 0.2911 - val_my_iou_metric: 0.7719\n",
      "\n",
      "Epoch 00023: val_my_iou_metric did not improve from 0.77544\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1326 - my_iou_metric: 0.8254 - val_loss: 0.3398 - val_my_iou_metric: 0.7556\n",
      "\n",
      "Epoch 00024: val_my_iou_metric did not improve from 0.77544\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1355 - my_iou_metric: 0.8180 - val_loss: 0.3078 - val_my_iou_metric: 0.7842\n",
      "\n",
      "Epoch 00025: val_my_iou_metric improved from 0.77544 to 0.78416, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1049 - my_iou_metric: 0.8355 - val_loss: 0.3168 - val_my_iou_metric: 0.7814\n",
      "\n",
      "Epoch 00026: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1119 - my_iou_metric: 0.8375 - val_loss: 0.3116 - val_my_iou_metric: 0.7784\n",
      "\n",
      "Epoch 00027: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1387 - my_iou_metric: 0.8192 - val_loss: 0.2921 - val_my_iou_metric: 0.7718\n",
      "\n",
      "Epoch 00028: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.1337 - my_iou_metric: 0.8292 - val_loss: 0.3268 - val_my_iou_metric: 0.7798\n",
      "\n",
      "Epoch 00029: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1077 - my_iou_metric: 0.8388 - val_loss: 0.2783 - val_my_iou_metric: 0.7670\n",
      "\n",
      "Epoch 00030: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1040 - my_iou_metric: 0.8385 - val_loss: 0.3259 - val_my_iou_metric: 0.7834\n",
      "\n",
      "Epoch 00031: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.1046 - my_iou_metric: 0.8423 - val_loss: 0.2945 - val_my_iou_metric: 0.7784\n",
      "\n",
      "Epoch 00032: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1292 - my_iou_metric: 0.8282 - val_loss: 0.3145 - val_my_iou_metric: 0.7227\n",
      "\n",
      "Epoch 00033: val_my_iou_metric did not improve from 0.78416\n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1223 - my_iou_metric: 0.8335 - val_loss: 0.3332 - val_my_iou_metric: 0.7878\n",
      "\n",
      "Epoch 00034: val_my_iou_metric improved from 0.78416 to 0.78778, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1072 - my_iou_metric: 0.8398 - val_loss: 0.2906 - val_my_iou_metric: 0.7762\n",
      "\n",
      "Epoch 00035: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.1060 - my_iou_metric: 0.8382 - val_loss: 0.3532 - val_my_iou_metric: 0.7415\n",
      "\n",
      "Epoch 00036: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0951 - my_iou_metric: 0.8571 - val_loss: 0.2938 - val_my_iou_metric: 0.7809\n",
      "\n",
      "Epoch 00037: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.0976 - my_iou_metric: 0.8560 - val_loss: 0.2644 - val_my_iou_metric: 0.7502\n",
      "\n",
      "Epoch 00038: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0939 - my_iou_metric: 0.8485 - val_loss: 0.2941 - val_my_iou_metric: 0.7867\n",
      "\n",
      "Epoch 00039: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0831 - my_iou_metric: 0.8532 - val_loss: 0.3297 - val_my_iou_metric: 0.7722\n",
      "\n",
      "Epoch 00040: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.0914 - my_iou_metric: 0.8559 - val_loss: 0.3433 - val_my_iou_metric: 0.7827\n",
      "\n",
      "Epoch 00041: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0893 - my_iou_metric: 0.8571 - val_loss: 0.2526 - val_my_iou_metric: 0.7519\n",
      "\n",
      "Epoch 00042: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1016 - my_iou_metric: 0.8337 - val_loss: 0.2990 - val_my_iou_metric: 0.7789\n",
      "\n",
      "Epoch 00043: val_my_iou_metric did not improve from 0.78778\n",
      "Epoch 44/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.0973 - my_iou_metric: 0.8525 - val_loss: 0.3783 - val_my_iou_metric: 0.7589\n",
      "\n",
      "Epoch 00044: val_my_iou_metric did not improve from 0.78778\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0741 - my_iou_metric: 0.8768 - val_loss: 0.2978 - val_my_iou_metric: 0.7926\n",
      "\n",
      "Epoch 00045: val_my_iou_metric improved from 0.78778 to 0.79264, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0693 - my_iou_metric: 0.8757 - val_loss: 0.2990 - val_my_iou_metric: 0.7930\n",
      "\n",
      "Epoch 00046: val_my_iou_metric improved from 0.79264 to 0.79302, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.0581 - my_iou_metric: 0.8882 - val_loss: 0.3035 - val_my_iou_metric: 0.7928\n",
      "\n",
      "Epoch 00047: val_my_iou_metric did not improve from 0.79302\n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0555 - my_iou_metric: 0.8911 - val_loss: 0.3114 - val_my_iou_metric: 0.7945\n",
      "\n",
      "Epoch 00048: val_my_iou_metric improved from 0.79302 to 0.79451, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0557 - my_iou_metric: 0.8857 - val_loss: 0.2948 - val_my_iou_metric: 0.7900\n",
      "\n",
      "Epoch 00049: val_my_iou_metric did not improve from 0.79451\n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0761 - my_iou_metric: 0.8725 - val_loss: 0.2890 - val_my_iou_metric: 0.7994\n",
      "\n",
      "Epoch 00050: val_my_iou_metric improved from 0.79451 to 0.79938, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0685 - my_iou_metric: 0.8814 - val_loss: 0.2692 - val_my_iou_metric: 0.7968\n",
      "\n",
      "Epoch 00051: val_my_iou_metric did not improve from 0.79938\n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0725 - my_iou_metric: 0.8849 - val_loss: 0.2921 - val_my_iou_metric: 0.7719\n",
      "\n",
      "Epoch 00052: val_my_iou_metric did not improve from 0.79938\n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0616 - my_iou_metric: 0.8812 - val_loss: 0.2989 - val_my_iou_metric: 0.8021\n",
      "\n",
      "Epoch 00053: val_my_iou_metric improved from 0.79938 to 0.80212, saving model to train_1010_adam_depth_aug_v3_v3_cv1.model\n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0709 - my_iou_metric: 0.8810 - val_loss: 0.3030 - val_my_iou_metric: 0.7794\n",
      "\n",
      "Epoch 00054: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0632 - my_iou_metric: 0.8862 - val_loss: 0.2825 - val_my_iou_metric: 0.7810\n",
      "\n",
      "Epoch 00055: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0596 - my_iou_metric: 0.8895 - val_loss: 0.3231 - val_my_iou_metric: 0.7844\n",
      "\n",
      "Epoch 00056: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 57/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0546 - my_iou_metric: 0.8946 - val_loss: 0.3035 - val_my_iou_metric: 0.7709\n",
      "\n",
      "Epoch 00057: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.0538 - my_iou_metric: 0.8960 - val_loss: 0.3096 - val_my_iou_metric: 0.7903\n",
      "\n",
      "Epoch 00058: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0551 - my_iou_metric: 0.8976 - val_loss: 0.3229 - val_my_iou_metric: 0.7842\n",
      "\n",
      "Epoch 00059: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.0561 - my_iou_metric: 0.8868 - val_loss: 0.3048 - val_my_iou_metric: 0.7901\n",
      "\n",
      "Epoch 00060: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 61/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0575 - my_iou_metric: 0.8925 - val_loss: 0.2890 - val_my_iou_metric: 0.7950\n",
      "\n",
      "Epoch 00061: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0494 - my_iou_metric: 0.8896 - val_loss: 0.3681 - val_my_iou_metric: 0.7823\n",
      "\n",
      "Epoch 00062: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0449 - my_iou_metric: 0.8975 - val_loss: 0.3819 - val_my_iou_metric: 0.7895\n",
      "\n",
      "Epoch 00063: val_my_iou_metric did not improve from 0.80212\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0485 - my_iou_metric: 0.9043 - val_loss: 0.3338 - val_my_iou_metric: 0.7944\n",
      "\n",
      "Epoch 00064: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0430 - my_iou_metric: 0.9048 - val_loss: 0.3284 - val_my_iou_metric: 0.7919\n",
      "\n",
      "Epoch 00065: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0471 - my_iou_metric: 0.9008 - val_loss: 0.3386 - val_my_iou_metric: 0.7884\n",
      "\n",
      "Epoch 00066: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0479 - my_iou_metric: 0.9062 - val_loss: 0.3112 - val_my_iou_metric: 0.7903\n",
      "\n",
      "Epoch 00067: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.0419 - my_iou_metric: 0.9030 - val_loss: 0.3247 - val_my_iou_metric: 0.7787\n",
      "\n",
      "Epoch 00068: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0399 - my_iou_metric: 0.9119 - val_loss: 0.3311 - val_my_iou_metric: 0.7889\n",
      "\n",
      "Epoch 00069: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.0439 - my_iou_metric: 0.9087 - val_loss: 0.3294 - val_my_iou_metric: 0.7852\n",
      "\n",
      "Epoch 00070: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0448 - my_iou_metric: 0.9074 - val_loss: 0.3096 - val_my_iou_metric: 0.7807\n",
      "\n",
      "Epoch 00071: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0436 - my_iou_metric: 0.9097 - val_loss: 0.3294 - val_my_iou_metric: 0.7830\n",
      "\n",
      "Epoch 00072: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0371 - my_iou_metric: 0.9119 - val_loss: 0.3349 - val_my_iou_metric: 0.7864\n",
      "\n",
      "Epoch 00073: val_my_iou_metric did not improve from 0.80212\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0363 - my_iou_metric: 0.9112 - val_loss: 0.3433 - val_my_iou_metric: 0.7838\n",
      "\n",
      "Epoch 00074: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0392 - my_iou_metric: 0.9152 - val_loss: 0.3359 - val_my_iou_metric: 0.7882\n",
      "\n",
      "Epoch 00075: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0352 - my_iou_metric: 0.9163 - val_loss: 0.3376 - val_my_iou_metric: 0.7868\n",
      "\n",
      "Epoch 00076: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.0377 - my_iou_metric: 0.9174 - val_loss: 0.3204 - val_my_iou_metric: 0.7794\n",
      "\n",
      "Epoch 00077: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.0372 - my_iou_metric: 0.9177 - val_loss: 0.3098 - val_my_iou_metric: 0.7853\n",
      "\n",
      "Epoch 00078: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0350 - my_iou_metric: 0.9196 - val_loss: 0.3014 - val_my_iou_metric: 0.7878\n",
      "\n",
      "Epoch 00079: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.0363 - my_iou_metric: 0.9185 - val_loss: 0.3296 - val_my_iou_metric: 0.7887\n",
      "\n",
      "Epoch 00080: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.0398 - my_iou_metric: 0.9182 - val_loss: 0.3167 - val_my_iou_metric: 0.7905\n",
      "\n",
      "Epoch 00081: val_my_iou_metric did not improve from 0.80212\n",
      "Epoch 82/200\n",
      " 22/200 [==>...........................] - ETA: 32s - loss: 0.0342 - my_iou_metric: 0.9155"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c1c1486d8b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                              ) \n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# No depth ver\n",
    "import warnings\n",
    "import gc\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "#model = UResNet34(input_shape=(128,128,3))\n",
    "\n",
    "ious = [0] * cv_total\n",
    "\n",
    "for cv_index in range(cv_total):\n",
    "    basic_name = '%s_v%s_cv%s' % (basic_name_ori,version,cv_index + 1)\n",
    "    print('############################################\\n', basic_name)\n",
    "    save_model_name = basic_name + '.model'\n",
    "    \n",
    "    print(\"make cv data...\")\n",
    "    warnings.filterwarnings('ignore')\n",
    "    x_train, y_train, x_valid, y_valid =  get_cv_data(cv_index+1)\n",
    "    \n",
    "    print(\"start data aug...\")\n",
    "    #Data augmentation\n",
    "    x_train_lr = np.array([np.fliplr(x) for x in x_train])\n",
    "    y_train_lr = np.array([np.fliplr(x) for x in y_train])\n",
    "    \n",
    "    x_train = np.vstack((x_train,x_train_lr))\n",
    "    y_train = np.vstack((y_train,y_train_lr))\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"start add channel...\")\n",
    "    x_train = np.repeat(x_train,3,axis=3)\n",
    "    x_valid = np.repeat(x_valid,3,axis=3)\n",
    "    \n",
    "    '''\n",
    "    print(\"start x_valid add channel...\")\n",
    "    # x_valid\n",
    "    valid_empty = np.zeros((len(x_valid),128,128,1))\n",
    "    for i in tqdm_notebook(range(len(valid_empty))):\n",
    "        for row, const in enumerate(np.linspace(0, 1, 128)):\n",
    "            valid_empty[i, row, :] = const\n",
    "\n",
    "    valid = x_valid * valid_empty\n",
    "\n",
    "    x_valid = np.append(x_valid,valid_empty,axis=3)\n",
    "    x_valid = np.append(x_valid,valid,axis=3)\n",
    "    '''\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"start bulid model...\")\n",
    "    #model = build_complie_model(lr = 0.0001)\n",
    "    model = UResNet34(input_shape=(128,128,3))\n",
    "    \n",
    "    c = Adam(lr = 0.0001)\n",
    "    model.compile(loss=bce_dice_loss, optimizer=c, metrics=[my_iou_metric])\n",
    "    csv_logger = CSVLogger(basic_name + '.log')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_my_iou_metric',patience=50,mode='max', verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',\n",
    "                                  factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    epochs = 200 #small number for demonstration \n",
    "    #batch_size = 32\n",
    "    steps_per_epoch = round(len(x_train)/batch_size,0)\n",
    "    valid_steps_per_epoch = round(len(x_valid)/batch_size,0)\n",
    "    \n",
    "    print(\"start fit_model...\")\n",
    "    history = model.fit_generator(generator(x_train, y_train, batch_size, seq_det),\n",
    "                              epochs=epochs,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data=(x_valid, y_valid),\n",
    "                              validation_steps= valid_steps_per_epoch,\n",
    "                              verbose = 1, \n",
    "                              callbacks = [model_checkpoint,csv_logger,reduce_lr,early_stopping],\n",
    "                              shuffle=True\n",
    "                             ) \n",
    "    \n",
    "    #plot_history(history,'my_iou_metric')\n",
    "    \n",
    "    model.load_weights(save_model_name)\n",
    "    \n",
    "    preds_valid = predict_result(model,x_valid,img_size_target)\n",
    "\n",
    "    preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "    y_valid_ori = np.array([downsample(x) for x in y_valid])\n",
    "    \n",
    "    \n",
    "    ious[cv_index] = get_iou_vector(y_valid_ori, (preds_valid > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv_index in range(cv_total):\n",
    "    print(\"cv %s ious = %s\" % (cv_index + 1,ious[cv_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best threholds\n",
    "# Create train/validation split stratified by salt coverage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    #np.array(train_df.images.map(padding).tolist()).reshape(-1, 128, 128, 1), \n",
    "    #np.array(train_df.masks.map(padding).tolist()).reshape(-1, 128, 128, 1),\n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.25, stratify=train_df.coverage_class, random_state= 8231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usedtime = 43.0 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# find best threholds\n",
    "# average the predictions from different folds\n",
    "t1 = time.time()\n",
    "preds_valid_all = np.zeros(np.squeeze(x_valid).shape)\n",
    "x_valid = np.repeat(x_valid,3,axis=3)\n",
    "\n",
    "for cv_index in range(cv_total):\n",
    "    basic_name = 'Unet_resnet_v%s_cv%s' % (version,cv_index + 1)\n",
    "    model.load_weights(basic_name + '.model')\n",
    "    preds_valid_all += predict_result(model,x_valid,img_size_target) /cv_total\n",
    "    #preds_test += predict_result(model,x_test,img_size_target) /cv_total\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"Usedtime = %s s\" % round((t2 - t1),1))\n",
    "\n",
    "preds_valid_all = np.array([downsample(x) for x in preds_valid_all])\n",
    "y_valid_ori = np.array([downsample(x) for x in y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 101, 101, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_valid_all.shape\n",
    "y_valid_ori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defalut metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "    \n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aad290070604d8c837ba1ea3247b987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Scoring for last model\n",
    "thresholds = np.linspace(0.1, 0.8, 31)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid_all > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f482dfc2dd8>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first train\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_test â†’ add dimãŒå¿…è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939148cb197349408ad06c061a3be4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "x_test = np.array([upsample(np.array(load_img(\"../input/images/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA only lr flip\n",
    "def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n",
    "    print('pred_1 start...')\n",
    "    preds_test = model.predict([x_test]).reshape(-1, img_size_target, img_size_target)\n",
    "        \n",
    "    print('pred_ref start...')\n",
    "    preds_test_reflect = np.array([np.fliplr(x) for x in model.predict([np.array([np.fliplr(x) for x in x_test])]).reshape(-1, img_size_target, img_size_target)])\n",
    "    preds_test += preds_test_reflect\n",
    "    \n",
    "    del preds_test_reflect\n",
    "    gc.collect()\n",
    "    \n",
    "    return preds_test / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106, 74, 37, 5]\n",
      "n_upsample_blocks:  5\n",
      "<keras.layers.core.Activation object at 0x7fdb74792f98>\n",
      "Tensor(\"stage3_unit4_relu2/Relu:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "<keras.layers.core.Activation object at 0x7fdb885d5f60>\n",
      "Tensor(\"stage3_unit1_relu1/Relu:0\", shape=(?, 16, 16, 128), dtype=float32)\n",
      "<keras.layers.core.Activation object at 0x7fdb966d9e80>\n",
      "Tensor(\"stage2_unit1_relu1/Relu:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
      "<keras.layers.core.Activation object at 0x7fde8a78bcc0>\n",
      "Tensor(\"relu0/Relu:0\", shape=(?, 64, 64, 64), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c33c7641d544209d43d6f35db000c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_1 start...\n",
      "pred_ref start...\n",
      "pred_updown start...\n",
      "pred_ref_updown start...\n",
      "ave result...\n",
      "(18000, 128, 128)\n",
      "pred_1 start...\n",
      "pred_ref start...\n",
      "pred_updown start...\n",
      "pred_ref_updown start...\n",
      "ave result...\n",
      "(18000, 128, 128)\n",
      "pred_1 start...\n",
      "pred_ref start...\n",
      "pred_updown start...\n",
      "pred_ref_updown start...\n",
      "ave result...\n",
      "(18000, 128, 128)\n",
      "pred_1 start...\n",
      "pred_ref start...\n",
      "pred_updown start...\n",
      "pred_ref_updown start...\n",
      "ave result...\n",
      "(18000, 128, 128)\n",
      "pred_1 start...\n",
      "pred_ref start...\n",
      "pred_updown start...\n",
      "pred_ref_updown start...\n",
      "ave result...\n",
      "(18000, 128, 128)\n",
      "\n",
      "Usedtime = 1312.7 s\n"
     ]
    }
   ],
   "source": [
    "# average the predictions from different folds\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "t1 = time.time()\n",
    "preds_test = np.zeros(np.squeeze(x_test).shape)\n",
    "model = UResNet34(input_shape=(128,128,3))\n",
    "\n",
    "for cv_index in tqdm_notebook(range(cv_total)):\n",
    "    gc.collect()\n",
    "    basic_name = 'Unet_resnet_v%s_cv%s' % (version,cv_index + 1)\n",
    "    model.load_weights(basic_name + '.model')\n",
    "    gc.collect()\n",
    "\n",
    "    preds_test += predict_result(model,x_test_3dim,img_size_target) / cv_total\n",
    "\n",
    "#preds_test_all = preds_test / cv_total\n",
    "t2 = time.time()\n",
    "print(\"Usedtime = %s s\" % round((t2 - t1),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffdae4f329b4773a61751cf7962a422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "#threshold_best  = 0.5 # some value in range 0.4- 0.5 may be better \n",
    "pred_dict = {idx: rle_encode(np.round(preds_test[i]) > threshold_best) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = 'Unet_resnet_5fold_20181007_adam_v1.csv'\n",
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unet_resnet_v1.csv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
